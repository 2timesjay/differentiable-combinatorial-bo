{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9984, 0.6346, 0.6157,  ..., 0.0390, 0.9799, 0.8945],\n",
      "        [0.4414, 0.2094, 0.3482,  ..., 0.9080, 0.4936, 0.1060],\n",
      "        [0.7729, 0.5911, 0.7002,  ..., 0.0216, 0.6762, 0.2952],\n",
      "        ...,\n",
      "        [0.7183, 0.4194, 0.7914,  ..., 0.5736, 0.4844, 0.3104],\n",
      "        [0.0254, 0.8569, 0.8162,  ..., 0.1626, 0.6628, 0.0663],\n",
      "        [0.5201, 0.3212, 0.9317,  ..., 0.0888, 0.2884, 0.3358]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from perturbations import perturbed\n",
    "\n",
    "dtype = torch.double\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "kwargs = {\"dtype\": dtype, \"device\": device}\n",
    "\n",
    "DEPTH = 4\n",
    "DIM = 2**DEPTH - 1\n",
    "\n",
    "path_inds = torch.zeros(DEPTH, DIM).long()\n",
    "path_inds[0] = torch.arange(start=0, end=DIM).long()\n",
    "path_inds[1] = ((path_inds[0]-1)/2).long()\n",
    "path_inds[2] = ((path_inds[1]-1)/2).long()\n",
    "path_inds[3] = ((path_inds[2]-1)/2).long()\n",
    "\n",
    "def best_path(inputs):\n",
    "    \"\"\"Returns the ranks of the input values among the given axis.\"\"\"\n",
    "    scores = inputs[..., path_inds].sum(dim=-2).squeeze(dim=-2)\n",
    "    best_scores_ind = scores[..., int(DIM/2):DIM].argmax(dim=-1, keepdim=True) + int(DIM/2)\n",
    "    best_path = torch.zeros_like(inputs)\n",
    "    # This handling of best_path_inds doesn't quite work as intended with 3+ dim input\n",
    "    best_path_inds = path_inds[...,best_scores_ind].squeeze(-1).transpose(-2, -1)\n",
    "    best_path.scatter_(dim=-1, index=best_path_inds, value=1.0)\n",
    "    return best_path\n",
    "\n",
    "# We initialize a random tensor\n",
    "x = torch.rand([128, DIM]).to(**kwargs)\n",
    "print(x)\n",
    "\n",
    "best_path(torch.rand(10, DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially, the values in our tensor do not result in the desired argsort\n",
      "tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Turn its grad on, since we will change this tensor to minimize our loss\n",
    "x.requires_grad = True\n",
    "y_true = torch.Tensor([1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "print(\"Initially, the values in our tensor do not result in the desired argsort\")\n",
    "print(best_path(x))\n",
    "print(y_true)\n",
    "\n",
    "torch.norm(perturbed(best_path, device=device)(x) - y_true, dim=-1)\n",
    "\n",
    "# FenchelYoungLoss(ranks, device=device)(y_true, x)\n",
    "\n",
    "# Initialize an SGD optimizer and do 200 steps\n",
    "optim = torch.optim.SGD([x], 1.0)\n",
    "# optim = torch.optim.LBFGS([x], .01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530.0\n",
      "1.5222587678283674\n",
      "396.0\n",
      "1.3643415413888658\n",
      "296.0\n",
      "1.2081967526769732\n",
      "236.0\n",
      "1.0552331247352917\n",
      "172.0\n",
      "0.9277827876056872\n",
      "150.0\n",
      "0.8039533755362521\n",
      "134.0\n",
      "0.7034852589045049\n",
      "110.0\n",
      "0.6208235629400555\n",
      "82.0\n",
      "0.5529975952964732\n",
      "74.0\n",
      "0.49107839130080644\n",
      "68.0\n",
      "0.4413605019335051\n",
      "SGD has run???\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "BASE = 10\n",
    "\n",
    "for iteration in range(10*BASE+1):\n",
    "    def closure():\n",
    "        optim.zero_grad()\n",
    "    #     criterion = FenchelYoungLoss(ranks, sigma=0.25, device=device)\n",
    "    #     loss = criterion(x, y_true).sum()\n",
    "        perturbed_x = perturbed(best_path, num_samples=100, sigma=0.25, noise='gumbel', device=device)(x)\n",
    "        loss = torch.norm(perturbed_x - y_true, dim=-1).mean()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optim.step(closure)\n",
    "    if iteration % BASE== 0:\n",
    "        perturbed_x = perturbed(best_path, num_samples=100, sigma=0.25, noise='gumbel', device=device)(x)\n",
    "        loss = torch.norm(perturbed_x - y_true, dim=-1).mean()\n",
    "#         print(perturbed_x)\n",
    "        print(torch.abs(best_path(x) - y_true).sum().item())\n",
    "        print(loss.item())\n",
    "        \n",
    "print(\"SGD has run???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128., 128.,   0., 121.,   7.,   0.,   0., 101.,  20.,   2.,   5.,   0.,\n",
       "          0.,   0.,   0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_path(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed Fixed Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5094, 0.6056, 0.0259,  ..., 0.3797, 0.8463, 0.0484],\n",
      "        [0.3547, 0.9234, 0.8487,  ..., 0.1251, 0.8251, 0.8643],\n",
      "        [0.4514, 0.4997, 0.9248,  ..., 0.0568, 0.8061, 0.1803],\n",
      "        ...,\n",
      "        [0.5334, 0.8792, 0.5630,  ..., 0.7294, 0.7334, 0.8180],\n",
      "        [0.0151, 0.3987, 0.3550,  ..., 0.4042, 0.2250, 0.9816],\n",
      "        [0.9972, 0.7213, 0.3172,  ..., 0.3397, 0.4160, 0.4699]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from perturbations import fixed_noise_perturbed, get_presampled_noises\n",
    "\n",
    "dtype = torch.double\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "kwargs = {\"dtype\": dtype, \"device\": device}\n",
    "\n",
    "DEPTH = 4\n",
    "DIM = 2**DEPTH - 1\n",
    "\n",
    "path_inds = torch.zeros(DEPTH, DIM).long()\n",
    "path_inds[0] = torch.arange(start=0, end=DIM).long()\n",
    "path_inds[1] = ((path_inds[0]-1)/2).long()\n",
    "path_inds[2] = ((path_inds[1]-1)/2).long()\n",
    "path_inds[3] = ((path_inds[2]-1)/2).long()\n",
    "\n",
    "def best_path(inputs):\n",
    "    \"\"\"Returns the ranks of the input values among the given axis.\"\"\"\n",
    "    scores = inputs[..., path_inds].sum(dim=-2).squeeze(dim=-2)\n",
    "    best_scores_ind = scores[..., int(DIM/2):DIM].argmax(dim=-1, keepdim=True) + int(DIM/2)\n",
    "    best_path = torch.zeros_like(inputs)\n",
    "    # This handling of best_path_inds doesn't quite work as intended with 3+ dim input\n",
    "    best_path_inds = path_inds[...,best_scores_ind].squeeze(-1).transpose(-2, -1)\n",
    "    best_path.scatter_(dim=-1, index=best_path_inds, value=1.0)\n",
    "    return best_path\n",
    "\n",
    "# We initialize a random tensor\n",
    "x = torch.rand([128, DIM]).to(**kwargs)\n",
    "print(x)\n",
    "\n",
    "best_path(torch.rand(10, DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially, the values in our tensor do not result in the desired argsort\n",
      "tensor([[1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Turn its grad on, since we will change this tensor to minimize our loss\n",
    "x.requires_grad = True\n",
    "y_true = torch.Tensor([1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "print(\"Initially, the values in our tensor do not result in the desired argsort\")\n",
    "print(best_path(x))\n",
    "print(y_true)\n",
    "\n",
    "presampled_noises = get_presampled_noises(x.shape, num_samples=1000, noise=\"normal\")\n",
    "torch.norm(\n",
    "    fixed_noise_perturbed(\n",
    "        best_path, \n",
    "        presampled_noises=presampled_noises, \n",
    "        num_samples=1000, \n",
    "        sigma=0.25, \n",
    "        device=device\n",
    "    )(x) - y_true, \n",
    "    dim=-1\n",
    ")\n",
    "\n",
    "# FenchelYoungLoss(ranks, device=device)(y_true, x)\n",
    "\n",
    "# Initialize an SGD optimizer and do 200 steps\n",
    "optim = torch.optim.SGD([x], 1.0)\n",
    "# optim = torch.optim.LBFGS([x], .1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "0.04560862041840651\n",
      "10.0\n",
      "0.0452581408411086\n",
      "10.0\n",
      "0.045151016317713635\n",
      "10.0\n",
      "0.04529177227512354\n",
      "10.0\n",
      "0.04501454448339936\n",
      "10.0\n",
      "0.0448788276075417\n",
      "10.0\n",
      "0.044686285659929596\n",
      "10.0\n",
      "0.04488033537437286\n",
      "10.0\n",
      "0.04476471284426817\n",
      "10.0\n",
      "0.044909720494043875\n",
      "10.0\n",
      "0.044411909550232165\n",
      "SGD has run???\n",
      "Wall time: 9.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "BASE = 10\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "for iteration in range(10*BASE+1):    \n",
    "    presampled_noises = get_presampled_noises(x, num_samples=N_SAMPLES, noise=\"normal\")\n",
    "    def closure():\n",
    "        optim.zero_grad()\n",
    "    #     criterion = FenchelYoungLoss(ranks, sigma=0.25, device=device)\n",
    "    #     loss = criterion(x, y_true).sum()\n",
    "        perturbed_x = fixed_noise_perturbed(\n",
    "            best_path, \n",
    "            presampled_noises=presampled_noises, \n",
    "            num_samples=N_SAMPLES, \n",
    "            sigma=0.25, \n",
    "            device=device\n",
    "        )(x)\n",
    "        loss = torch.norm(perturbed_x - y_true, dim=-1).mean()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optim.step(closure)\n",
    "    if iteration % BASE== 0:\n",
    "        perturbed_x = fixed_noise_perturbed(\n",
    "            best_path, \n",
    "            presampled_noises=presampled_noises, \n",
    "            num_samples=N_SAMPLES, \n",
    "            sigma=0.25, \n",
    "            device=device\n",
    "        )(x)\n",
    "        loss = torch.norm(perturbed_x - y_true, dim=-1).mean()\n",
    "#         print(perturbed_x)\n",
    "        print(torch.abs(best_path(x) - y_true).sum().item())\n",
    "        print(loss.item())\n",
    "        \n",
    "print(\"SGD has run???\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVXPyLayers - DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import torch\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convex optimization model\n",
    "num_n = 5\n",
    "num_e = 5\n",
    "e_out = [\n",
    "    [0, 1],\n",
    "    [2, 3],\n",
    "    [4],\n",
    "    [],\n",
    "    [],\n",
    "]\n",
    "e_in = [\n",
    "    [],\n",
    "    [0],\n",
    "    [1],\n",
    "    [2, 4],\n",
    "    [3],\n",
    "]\n",
    "\n",
    "n = cp.Variable(num_n)\n",
    "e = cp.Variable(num_e)\n",
    "s_in = cp.Parameter(1)\n",
    "s_out = cp.Variable(2)\n",
    "e_hat = cp.Parameter(num_e)\n",
    "\n",
    "objective = cp.norm2(e-e_hat)\n",
    "bound_constraints = [e >= 0, e <= 1]\n",
    "flow_constraints = [\n",
    "    s_in[0] - e[0] - e[1] == 0,\n",
    "    e[0] - e[2] - e[3] == 0,\n",
    "    e[1] - e[4] == 0,\n",
    "    e[2] + e[4] + s_out[0] == 0,\n",
    "    e[3] + s_out[1] == 0,\n",
    "    \n",
    "]\n",
    "source_sink_constraints = [\n",
    "    s_in[0] + s_out[0] + s_out[1] == 0,\n",
    "]\n",
    "constraints = bound_constraints + flow_constraints + source_sink_constraints\n",
    "\n",
    "prob = cp.Problem(objective=cp.Minimize(objective), constraints=constraints)\n",
    "dag_proj_layer = CvxpyLayer(problem=prob, parameters=[e_hat, s_in], variables=[e, s_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.2500e-01,  3.7500e-01,  6.2500e-01, -1.2694e-10,  3.7500e-01],\n",
       "       grad_fn=<_CvxpyLayerFnFnBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer(torch.Tensor([1.0]), torch.Tensor([1.0, 0.5]))\n",
    "path_true = torch.Tensor([1.0, 0.0, 0.0, 1.0, 0.0])\n",
    "e_arg = torch.Tensor([1.5, 1.0, 1.0, 0.0, 1.0])\n",
    "e_arg.requires_grad = True\n",
    "s_in_arg = torch.tensor([1.0])\n",
    "e_res, s_res = dag_proj_layer(e_arg, s_in_arg)\n",
    "loss = torch.norm(e_res - path_true)\n",
    "loss.backward()\n",
    "e_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.2500e-01,  3.7500e-01,  6.2500e-01, -1.2694e-10,  3.7500e-01],\n",
       "         [ 1.0000e+00,  1.0000e+00,  9.9997e-01,  3.6081e-05,  1.0000e+00]]),\n",
       " tensor([[-1.0000e+00,  1.6819e-10],\n",
       "         [-2.0000e+00, -3.3548e-05]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag_proj_layer(torch.Tensor([e_arg.tolist(), e_arg.tolist()]), torch.Tensor([[1.0], [2.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.double\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "kwargs = {\"dtype\": dtype, \"device\": device}\n",
    "\n",
    "DIM = 5\n",
    "\n",
    "# We initialize a random tensor\n",
    "x = torch.rand([128, DIM]).to(**kwargs)\n",
    "print(x.shape)\n",
    "source = torch.ones(x.shape[0], 1).to(**kwargs)\n",
    "print(source.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 1., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Turn its grad on, since we will change this tensor to minimize our loss\n",
    "x.requires_grad = True\n",
    "y_true = torch.Tensor([1.0, 0.0, 0.0, 1.0, 0.0]).to(**kwargs)\n",
    "\n",
    "print(y_true)\n",
    "\n",
    "# Initialize an SGD optimizer and do 200 steps\n",
    "optim = torch.optim.Adam([x], 1.0)\n",
    "# optim = torch.optim.LBFGS([x], lr=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.000512770950756\n",
      "0.08191826200046354 1.6599763749581091\n",
      "16.002033550906674\n",
      "0.0819249876632022 1.0906809954782428\n",
      "16.00333259463037\n",
      "0.08193021047826303 0.6494827182949718\n",
      "16.001760518178287\n",
      "0.08192345153914424 0.2888068104523683\n",
      "16.00227371371424\n",
      "0.08192531921804438 -0.015194829761478186\n",
      "16.002672576694863\n",
      "0.08192671455432253 -0.2770450974642895\n",
      "16.002848632752467\n",
      "0.0819273362390188 -0.5055346821213603\n",
      "16.003573304081602\n",
      "0.0819306215730234 -0.7068369512523363\n",
      "16.003164003461393\n",
      "0.08192879223237422 -0.8855295932813281\n",
      "16.00210037377061\n",
      "0.08192460644265018 -1.0452757951581344\n",
      "16.001309180195705\n",
      "0.08192152458448611 -1.1883273407460293\n",
      "SGD has run???\n",
      "TIMES:  0 0\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import time\n",
    "BASE = 1\n",
    "\n",
    "fwd = 0\n",
    "bwd = 0\n",
    "for iteration in range(10*BASE+1):\n",
    "    \n",
    "    def closure():\n",
    "        optim.zero_grad()\n",
    "        start = time.time()\n",
    "        dag_proj, _ = dag_proj_layer(x, source)\n",
    "        end = time.time()\n",
    "#         fwd += end - start\n",
    "        loss = torch.norm(dag_proj - y_true, dim=-1).mean()\n",
    "    #     + torch.norm(dag_proj - x)\n",
    "    #     + torch.maximum(\n",
    "    #         torch.norm(x, dim=-1) - torch.Tensor([6]).to(**kwargs), torch.Tensor([0]).to(**kwargs)\n",
    "    #     ).mean()\n",
    "        start = time.time()\n",
    "        loss.backward()\n",
    "        end = time.time()\n",
    "#         bwd += end - start  \n",
    "        return loss\n",
    "    optim.step(closure)\n",
    "    if iteration % BASE== 0:\n",
    "        dag_proj, _ = dag_proj_layer(x, source)\n",
    "        loss = torch.norm(dag_proj - y_true, dim=-1).mean()\n",
    "#         print(perturbed_x)\n",
    "#         print(dag_proj[:10,:7])\n",
    "        print(torch.abs(dag_proj - y_true).sum().item())\n",
    "        print(loss.item(), torch.sum(x, dim=-1).mean().item())\n",
    "        \n",
    "print(\"SGD has run???\")\n",
    "print(\"TIMES: \", fwd, bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([127.,   1.,   6., 121.,   1.], dtype=torch.float64,\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(dag_proj).sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVXPyLayers - Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.double\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "kwargs = {\"dtype\": dtype, \"device\": device}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convex optimization model\n",
    "num_e = 14\n",
    "num_sinks = 8\n",
    "e = cp.Variable(num_e)\n",
    "s_in = cp.Parameter(1)\n",
    "s_out = cp.Variable(num_sinks)\n",
    "e_hat = cp.Parameter(num_e)\n",
    "\n",
    "objective = cp.norm2(e-e_hat)\n",
    "bound_constraints = [e >= 0, e <= 1]\n",
    "flow_constraints = [\n",
    "    e[0] - e[2] - e[3] == 0,\n",
    "    e[1] - e[4] - e[5] == 0,\n",
    "    e[2] - e[6] - e[7] == 0,\n",
    "    e[3] - e[8] - e[9] == 0,\n",
    "    e[4] - e[10] - e[11] == 0,\n",
    "    e[5] - e[12] - e[13] == 0,\n",
    "] + [\n",
    "    e[6+i] + s_out[i] == 0\n",
    "    for i in range(num_sinks)\n",
    "]\n",
    "source_sink_constraints = [\n",
    "    s_in[0] + cp.sum(s_out) == 0,\n",
    "]\n",
    "constraints = bound_constraints + flow_constraints + source_sink_constraints\n",
    "\n",
    "prob = cp.Problem(objective=cp.Minimize(objective), constraints=constraints)\n",
    "dag_proj_layer = CvxpyLayer(problem=prob, parameters=[e_hat, s_in], variables=[e, s_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 14])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "DIM = 14\n",
    "\n",
    "# We initialize a random tensor\n",
    "x = torch.rand([128, DIM]).to(**kwargs)\n",
    "print(x.shape)\n",
    "source = torch.ones(x.shape[0], 1).to(**kwargs)\n",
    "print(source.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor(1.5366, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Turn its grad on, since we will change this tensor to minimize our loss\n",
    "x.requires_grad = True\n",
    "y_true = torch.Tensor([1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "# print(best_path(x))\n",
    "print(y_true)\n",
    "\n",
    "dag_proj, _ = dag_proj_layer(x, source) \n",
    "print(torch.norm(dag_proj - y_true, dim=-1).mean())\n",
    "\n",
    "# Initialize an SGD optimizer and do 200 steps\n",
    "# optim = torch.optim.SGD([x], lr=10.0)\n",
    "optim = torch.optim.LBFGS([x], lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "88.00155916694962\n",
      "0.4408341696149421 7.011729437540335\n",
      "SGD has run???\n",
      "TIMES:  0 0\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "BASE = 1\n",
    "\n",
    "fwd = 0\n",
    "bwd = 0\n",
    "for iteration in range(10*BASE+1):\n",
    "    \n",
    "    def closure():\n",
    "        optim.zero_grad()\n",
    "        start = time.time()\n",
    "        dag_proj, _ = dag_proj_layer(x, source)\n",
    "        end = time.time()\n",
    "#         fwd += end - start\n",
    "        loss = torch.norm(dag_proj - y_true, dim=-1).mean()\n",
    "    #     + torch.norm(dag_proj - x)\n",
    "    #     + torch.maximum(\n",
    "    #         torch.norm(x, dim=-1) - torch.Tensor([6]).to(**kwargs), torch.Tensor([0]).to(**kwargs)\n",
    "    #     ).mean()\n",
    "        start = time.time()\n",
    "        loss.backward()\n",
    "        end = time.time()\n",
    "#         bwd += end - start  \n",
    "        return loss\n",
    "    optim.step(closure)\n",
    "    if iteration % BASE== 0:\n",
    "        dag_proj, _ = dag_proj_layer(x, source)\n",
    "        loss = torch.norm(dag_proj - y_true, dim=-1).mean()\n",
    "#         print(perturbed_x)\n",
    "#         print(dag_proj[:10,:7])\n",
    "        print(torch.abs(dag_proj - y_true).sum().item())\n",
    "        print(loss.item(), torch.sum(x, dim=-1).mean().item())\n",
    "        \n",
    "print(\"SGD has run???\")\n",
    "print(\"TIMES: \", fwd, bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3032e+01, 1.2272e-03, 3.5869e+01, 2.7268e+01, 4.7818e-01, 3.0829e+01,\n",
       "        6.7247e-01, 2.5717e+01, 3.3426e-01, 1.5892e-02, 1.4308e-01, 1.1910e-01,\n",
       "        3.0708e-01, 4.4536e-02, 3.3555e-01, 7.5614e-01, 2.2788e+01, 2.5765e+01,\n",
       "        2.7447e+01, 3.1402e+01, 2.7189e+01, 3.0408e+01, 2.7166e+01, 8.9052e-02,\n",
       "        2.5729e+01, 2.2520e-01, 1.8217e-02, 2.5697e+01, 2.9008e+01, 1.7795e-01,\n",
       "        5.9570e-01, 2.7840e+01, 2.9448e+01, 2.6707e+01, 2.4076e+01, 2.7410e+01,\n",
       "        2.3254e+01, 2.2695e+01, 2.4997e-03, 2.1016e-02, 2.6753e+01, 2.5013e+01,\n",
       "        2.7842e+01, 2.8003e+01, 2.4093e+01, 1.7035e-01, 2.7923e+01, 2.9427e+01,\n",
       "        2.7241e+01, 2.7868e+01, 3.0894e+01, 2.5651e-01, 8.0247e-02, 2.6379e-01,\n",
       "        2.6699e+01, 6.2519e-01, 3.7394e-01, 3.2489e+01, 2.5667e+01, 2.5440e+01,\n",
       "        2.7112e+01, 2.5906e+01, 2.4713e+01, 2.7892e+01, 2.5580e+01, 2.6652e+01,\n",
       "        3.1305e+01, 3.4211e+01, 2.6799e+01, 2.6907e+01, 2.6111e+01, 3.5460e+01,\n",
       "        2.5712e+01, 2.0993e+01, 2.6971e+01, 1.7052e-01, 2.7325e+01, 2.6610e+01,\n",
       "        3.0795e+01, 3.1145e+01, 3.1333e+01, 2.4982e+01, 2.4851e+01, 2.4814e+01,\n",
       "        2.6892e+01, 2.6804e+01, 2.7395e+01, 1.2271e-01, 2.8772e+01, 2.3782e+01,\n",
       "        2.7852e+01, 2.5617e+01, 3.2038e+01, 2.6573e+01, 3.0011e+01, 2.6902e+01,\n",
       "        3.3222e-01, 2.4479e+01, 6.4192e-02, 2.5492e-01, 2.4362e+01, 2.5960e+01,\n",
       "        1.1956e-01, 2.6209e+01, 2.7142e+01, 3.1096e+01, 2.6464e+01, 2.6804e+01,\n",
       "        2.8953e+01, 2.5543e+01, 2.5516e+01, 1.8840e-01, 2.6935e+01, 2.6981e+01,\n",
       "        2.1077e+01, 4.8201e-02, 2.4965e+01, 3.1145e+01, 2.1503e+01, 7.0928e-01,\n",
       "        5.8104e-03, 3.2505e-01, 2.5479e+01, 1.8246e-01, 5.0868e-01, 3.0689e+01,\n",
       "        2.8338e+01, 3.2260e+01], dtype=torch.float64, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00, -1.0554e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00, -5.6148e-10],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 9.9999e-01, -9.9960e-07, -1.3740e-06],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00, -5.3321e-09, -1.3239e-08],\n",
       "        [ 1.0000e+00,  1.0000e+00, -9.5956e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  4.2307e-10],\n",
       "        [ 1.0000e+00,  1.0000e+00, -4.5745e-10],\n",
       "        [ 1.0000e+00,  1.0000e+00,  7.7381e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  2.1403e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  6.7369e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.5159e-07],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  9.5009e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  2.7052e-09],\n",
       "        [ 9.9999e-01,  1.0000e+00,  6.9925e-06],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00, -1.1429e-10, -3.4759e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0579e-10],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00, -1.0606e-09],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.4494e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00, -1.5438e-10],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00, -5.3263e-10],\n",
       "        [ 1.0000e+00,  9.9998e-01, -6.1263e-06],\n",
       "        [ 1.0000e+00,  1.0000e+00, -1.1399e-10],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  8.3982e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  7.2818e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 9.9999e-01,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.9159e-10],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0001e+00,  1.0001e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00, -4.8927e-13, -1.1515e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00, -4.7055e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00, -3.1588e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00, -4.3102e-12],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  3.8216e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  7.5019e-06],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  2.2191e-07,  3.0567e-07],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  9.9999e-01],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  2.2635e-11, -6.5938e-12],\n",
       "        [ 1.0000e+00,  1.0000e+00,  2.6616e-11],\n",
       "        [ 1.0000e+00,  1.0000e+00,  8.2794e-12],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  2.0229e-09],\n",
       "        [ 1.0000e+00,  3.1386e-11, -8.6270e-13],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00]], dtype=torch.float64,\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag_proj[:,[0,2,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
